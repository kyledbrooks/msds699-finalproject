{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "annual-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "nonprofit-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm             import SVC\n",
    "\n",
    "from sklearn.decomposition   import PCA\n",
    "\n",
    "import imblearn\n",
    "from   imblearn.pipeline          import make_pipeline # scikit-learn Pipeline does not work with imblearn\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, fbeta_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fitted-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alike-faculty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0  75.0        0                       582         0                 20   \n",
       "1  55.0        0                      7861         0                 38   \n",
       "2  65.0        0                       146         0                 20   \n",
       "3  50.0        1                       111         0                 20   \n",
       "4  65.0        1                       160         1                 20   \n",
       "\n",
       "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                    1  265000.00               1.9           130    1   \n",
       "1                    0  263358.03               1.1           136    1   \n",
       "2                    0  162000.00               1.3           129    1   \n",
       "3                    0  210000.00               1.9           137    1   \n",
       "4                    0  327000.00               2.7           116    0   \n",
       "\n",
       "   smoking  time  DEATH_EVENT  \n",
       "0        0     4            1  \n",
       "1        0     6            1  \n",
       "2        1     7            1  \n",
       "3        0     7            1  \n",
       "4        0     8            1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/heart_failure_clinical_records_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mounted-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.DEATH_EVENT # Our target variable is DEATH_EVENT \n",
    "X = data.drop(columns='DEATH_EVENT')  # Remove the target variable from X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "congressional-engine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many observations do we have?\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-darkness",
   "metadata": {},
   "source": [
    "After importing the data, the next step is to pull off a segment of the data that will be our testing set. This test set will be be hidden away from the models we are designing until we are ready to test one. Additionally, since there are only have 299 observations, I am going to split the data by a higher percentage to ensure that the testing set is somewhat representative of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "critical-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing split that we are going to hide away from our model\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "occasional-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-juvenile",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "catholic-friendly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 209 entries, 85 to 57\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       209 non-null    float64\n",
      " 1   anaemia                   209 non-null    int64  \n",
      " 2   creatinine_phosphokinase  209 non-null    int64  \n",
      " 3   diabetes                  209 non-null    int64  \n",
      " 4   ejection_fraction         209 non-null    int64  \n",
      " 5   high_blood_pressure       209 non-null    int64  \n",
      " 6   platelets                 209 non-null    float64\n",
      " 7   serum_creatinine          209 non-null    float64\n",
      " 8   serum_sodium              209 non-null    int64  \n",
      " 9   sex                       209 non-null    int64  \n",
      " 10  smoking                   209 non-null    int64  \n",
      " 11  time                      209 non-null    int64  \n",
      "dtypes: float64(3), int64(9)\n",
      "memory usage: 21.2 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-funeral",
   "metadata": {},
   "source": [
    "One can see that there are 12 columns in the data, and they all appear to have numeric data-types. However, some of the columns actually are actually categorical andd consist of 0s and 1s (corresponding to whether or not a specific patient falls into that category or not). In summary, these categorical columns have already been one-hot encoded, so there is no need to incorporate that into the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "coated-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n",
    "con_cols = ['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'age', 'time']\n",
    "age_col = ['age']  # Split off age from con_cols since there MinMaxScaler(0,1) works well for ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "individual-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_pipe = Pipeline([('scaler', StandardScaler()),  \n",
    "                     ('imputer', SimpleImputer(strategy='median', add_indicator=True))])\n",
    "\n",
    "# con_pipe = Pipeline([('scaler', StandardScaler()),  \n",
    "#                      ('imputer', IterativeImputer())])\n",
    "\n",
    "# age_pipe = Pipeline([('scaler', MinMaxScaler((0,1))),  # MinMax Scaler good to apply to age\n",
    "#                      ('imputer', SimpleImputer(strategy='median', add_indicator=True))])\n",
    "\n",
    "cat_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('imputer', SimpleImputer(strategy='most_frequent'))])\n",
    "\n",
    "# cat_pipe = Pipeline([('imputer', IterativeImputer())])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_cols),\n",
    "                                   ('continuous', con_pipe, con_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "clinical-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con_pipe = make_pipeline(imblearn.over_sampling.SMOTE(), StandardScaler(), SimpleImputer(strategy='median'))\n",
    "\n",
    "# age_pipe = make_pipeline(imblearn.over_sampling.SMOTE(), MinMaxScaler((0,1)), SimpleImputer(strategy='median'))\n",
    "\n",
    "# cat_pipe = make_pipeline(imblearn.over_sampling.SMOTE(), SimpleImputer(strategy='most_frequent'))\n",
    "\n",
    "# preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_cols),\n",
    "#                                    ('continuous', con_pipe, con_cols),\n",
    "#                                    ('age', age_pipe, age_col)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-terror",
   "metadata": {},
   "source": [
    "# Algorithms & Search\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "beginning-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "diverse-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe = make_pipeline(imblearn.over_sampling.SMOTE(),\n",
    "#                      preprocessing,\n",
    "#                      PCA(),\n",
    "#                      DummyEstimator()\n",
    "#                      )\n",
    "\n",
    "pipe = Pipeline(steps = [('preprocessing', preprocessing),\n",
    "                         ('dummyestimator', DummyEstimator())])\n",
    "\n",
    "\n",
    "search_space = [\n",
    "    {'dummyestimator': [LogisticRegression(n_jobs=-1)],\n",
    "        'dummyestimator__C': np.linspace(0.01, 5, 10),\n",
    "        'dummyestimator__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'dummyestimator__class_weight': ['balanced', None],\n",
    "        'dummyestimator__penalty': ['l1', 'l2', 'elasticnet', 'none']},\n",
    "    \n",
    "    {'dummyestimator': [RandomForestClassifier(n_jobs=-1)],\n",
    "        'dummyestimator__criterion': ['gini', 'entropy'],\n",
    "        'dummyestimator__min_samples_leaf': np.linspace(1, 10, 4, dtype=int),\n",
    "        'dummyestimator__bootstrap': [True, False],\n",
    "        'dummyestimator__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "        'dummyestimator__n_estimators': np.linspace(50, 300, 5, dtype=int)},\n",
    "    \n",
    "    {'dummyestimator': [SVC()], \n",
    "         'dummyestimator__class_weight': ['balanced', None],\n",
    "         'dummyestimator__C': np.linspace(1, 100, 10),\n",
    "         'dummyestimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "         'dummyestimator__degree': range(1,10)}, \n",
    "    \n",
    "    {'dummyestimator': [KNeighborsClassifier(n_jobs=-1)],\n",
    "         'dummyestimator__leaf_size': np.linspace(5, 50, 5, dtype=int),\n",
    "         'dummyestimator__n_neighbors': np.linspace(3, 13, 4, dtype=int),\n",
    "         'dummyestimator__weights': ['uniform', 'distance'],\n",
    "         'dummyestimator__p': [1,2]}, \n",
    "    \n",
    "    {'dummyestimator': [GaussianNB()]}, \n",
    "    \n",
    "    {'dummyestimator': [ExtraTreesClassifier(n_jobs=-1)], \n",
    "         'dummyestimator__criterion': ['gini', 'entropy'],\n",
    "         'dummyestimator__min_samples_leaf': np.linspace(1, 30, 5, dtype=int),\n",
    "         'dummyestimator__bootstrap': [True, False],\n",
    "         'dummyestimator__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "         'dummyestimator__n_estimators': np.linspace(50, 500, 8, dtype=int)}\n",
    "]\n",
    "\n",
    "\n",
    "# print('''search_space = [\n",
    "#     {'dummyestimator': [LogisticRegression(n_jobs=-1)],\n",
    "#         'dummyestimator__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "#         'clf__class_weight': ['balanced', None],\n",
    "#         'clf__penalty': ['l1', 'l2', 'elasticnet', 'none']},\n",
    "    \n",
    "#     {'dummyestimator': [RandomForestClassifier(n_jobs=-1)],\n",
    "#         'clf__criterion': ['gini', 'entropy'],\n",
    "#         'clf__min_samples_leaf': np.linspace(1, 10, 4, dtype=int),\n",
    "#         'clf__bootstrap': [True, False],\n",
    "#         'clf__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "#         'clf__n_estimators': np.linspace(50, 300, 5, dtype=int)},\n",
    "    \n",
    "#     {'dummyestimator': [SVC()], \n",
    "#          'clf__class_weight': ['balanced', None],\n",
    "#          'clf__C': np.linspace(1, 100, 10),\n",
    "#          'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "#          'clf__degree': range(1,10)}, \n",
    "    \n",
    "#     {'dummyestimator': [KNeighborsClassifier(n_jobs=-1)],\n",
    "#          'clf__leaf_size': np.linspace(5, 50, 5, dtype=int),\n",
    "#          'clf__n_neighbors': np.linspace(3, 13, 4, dtype=int),\n",
    "#          'clf__weights': ['uniform', 'distance'],\n",
    "#          'clf__p': [1,2]}\n",
    "# ]'''.replace('clf__', 'dummyestimator__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "departmental-calendar",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8436796864430655,\n",
       " {'dummyestimator__n_estimators': 300,\n",
       "  'dummyestimator__min_samples_leaf': 10,\n",
       "  'dummyestimator__criterion': 'gini',\n",
       "  'dummyestimator__class_weight': 'balanced_subsample',\n",
       "  'dummyestimator__bootstrap': True,\n",
       "  'dummyestimator': RandomForestClassifier(class_weight='balanced_subsample', min_samples_leaf=10,\n",
       "                         n_estimators=300, n_jobs=-1)})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1_weighted', \n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "gs.best_score_, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "mediterranean-saturday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced_subsample', min_samples_leaf=10,\n",
       "                       n_estimators=300, n_jobs=-1)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gs.best_params_['dummyestimator']\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-rwanda",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-diagram",
   "metadata": {},
   "source": [
    "Now that I have our ideal model based on automated hyperparameter search and model selection, next I look at a variety of evaluation metrics on the training data to assess our models performance on the training set. The first metric I looked at was the weighted f1 score since that is what I used for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "continental-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [('preprocessing', preprocessing), \n",
    "                         ('clf', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "digital-plain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991584936439413"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "desperate-detection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991584936439413"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-ivory",
   "metadata": {},
   "source": [
    "I chose to look at a fbeta_score metric with a beta=2. I chose this metric since in a business setting use of this ML model deals with human lives. Therefore, this model should focus on reducing the amount of false negatives (we would rather tell someone they are at risk of a death event when they're not than miss identifying a person who has a death event. For this reason, I chose to look at fbeta_score with a beta value greater than 1 (as this puts more emphasis on false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "lightweight-position",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8793103448275862"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(y_train, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-updating",
   "metadata": {},
   "source": [
    "Another evaluation metric I chose to look at "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
