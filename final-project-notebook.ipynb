{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rental-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "nonprofit-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm             import SVC\n",
    "\n",
    "from sklearn.decomposition   import PCA\n",
    "\n",
    "import imblearn\n",
    "from   imblearn.pipeline          import make_pipeline # scikit-learn Pipeline does not work with imblearn\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, fbeta_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-cameroon",
   "metadata": {},
   "source": [
    "# Data Science Research Question\n",
    "-----\n",
    "## Can we develop an ML model to predict whether or not a patient will have a death event based on common heart failure predictors? Additionally, what are the most important predictors of a death event?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alike-faculty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0  75.0        0                       582         0                 20   \n",
       "1  55.0        0                      7861         0                 38   \n",
       "2  65.0        0                       146         0                 20   \n",
       "3  50.0        1                       111         0                 20   \n",
       "4  65.0        1                       160         1                 20   \n",
       "\n",
       "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                    1  265000.00               1.9           130    1   \n",
       "1                    0  263358.03               1.1           136    1   \n",
       "2                    0  162000.00               1.3           129    1   \n",
       "3                    0  210000.00               1.9           137    1   \n",
       "4                    0  327000.00               2.7           116    0   \n",
       "\n",
       "   smoking  time  DEATH_EVENT  \n",
       "0        0     4            1  \n",
       "1        0     6            1  \n",
       "2        1     7            1  \n",
       "3        0     7            1  \n",
       "4        0     8            1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/heart_failure_clinical_records_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-crest",
   "metadata": {},
   "source": [
    "First after very basic EDA, I summarize the features in this dataset, and how they relate to heart failure:\n",
    "- **Age** - age of patient in years\n",
    "- **Anaemia** - whether or not a patient has a decrease of red blood cells (boolean)\n",
    "- **Creatinine Phosphokinase** - level of CPK enzyme in blood in (high values may indicate muscle damage - heart is a muscle)\n",
    "- **Diabetes** - whether or not a patient has diabetes (boolean)\n",
    "- **Ejection Fraction** - percentage of blood being pumped out of left ventricle (lower values may indicate issues)\n",
    "- **High Blood Pressure** - whether or not a patient has hypertension (boolean)\n",
    "- **Platelets** - concentration of platelets in blood in kiloplatelets/mL\n",
    "- **Serum Creatinine** - concentration of creatinine in blood in mg/dL (increased levels are a marker of poor cardiac output).\n",
    "- **Serum Sodium** - level of sodium in blood in mEq/L (low concentration is a biological marker for heart failure)\n",
    "- **Sex** - whether the patient is male or female (binary: 1=Male, 0=Female)\n",
    "- **Smoking** - if the patient smokes (boolean)\n",
    "- **Time** - the follow up period for the patient up to the death event (whether they died or were censored).\n",
    "- **Death Event** - (target), whether or not the patient died during the follow up (boolean)\n",
    "    - 1 - patient died\n",
    "    - 0 - patient was censored (scientist lost contact with patient) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-fault",
   "metadata": {},
   "source": [
    "Since the time variable is populated when the scientists either lost contact with the patient, or the patient died, it is indicative of survival, and obviously not known beforehand. In other words, if we we're to deploy a working model with all the columns and someone wanted to predict whether a certain patient was likely to have a death event, they would have no \"time\" variable to put as an input (since that is recorded when the patient dies). Therefore, I made the decision initially to drop time as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mounted-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.DEATH_EVENT # Our target variable is DEATH_EVENT \n",
    "X = data.drop(columns=['DEATH_EVENT', 'time'])  # Remove the target variable and time from X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "congressional-engine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many observations do we have?\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-darkness",
   "metadata": {},
   "source": [
    "After importing the data, the next step is to pull off a segment of the data that will be our testing set. This test set will be be hidden away from the models we are designing until we are ready to test one. Additionally, since there are only have 299 observations, I am going to split the data by a slightly higher percentage (25% as opposed to 20%) to ensure that the testing set is somewhat representative of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "critical-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing split that we are going to hide away from our model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-juvenile",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-glucose",
   "metadata": {},
   "source": [
    "After performing more extensive EDA (in a different notebook), I noticed that all of the columns were numeric types, but some represented categorical variables (i.e. whether or not someone smokes, has high blood pressure, etc.). In the dataset, these columns are already one-hot-encoded with binary 1s or 0s to indicate whether someone exhibits that characteristic. Therefore, in my feature engineering one-hot-encoding is not needed. Additionally, there are no nan values in the training data, however I still impute for missing values, as they might be included in the testing set or data we want to use the model on.\n",
    "\n",
    "For my feature engineering, the steps I took were:\n",
    "1. Identify categorical and numerical columns and establish pipelines for each\n",
    "\n",
    "\n",
    "2. Scale the data\n",
    "    - For my numeric data I chose to use StandardScaler(), which normalizes columns to fall in line with the normal distribution. This is important as many algorithms may perform poorly if individual features do not look more or less like a standard normally distributed data.\n",
    "    - It doesn't make sense to use a scaler on my categorical columns since they're simply one-hot-encoded as 1s or 0s.\n",
    "\n",
    "\n",
    "3. Impute missing values with SimpleImputer()\n",
    "    - For numerical columns I used the strategy \"median\" whereas for categorical I used \"most_frequent\". These methods fill any missing values with the corresponding strategy applied to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "catholic-friendly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299 entries, 0 to 298\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       299 non-null    float64\n",
      " 1   anaemia                   299 non-null    int64  \n",
      " 2   creatinine_phosphokinase  299 non-null    int64  \n",
      " 3   diabetes                  299 non-null    int64  \n",
      " 4   ejection_fraction         299 non-null    int64  \n",
      " 5   high_blood_pressure       299 non-null    int64  \n",
      " 6   platelets                 299 non-null    float64\n",
      " 7   serum_creatinine          299 non-null    float64\n",
      " 8   serum_sodium              299 non-null    int64  \n",
      " 9   sex                       299 non-null    int64  \n",
      " 10  smoking                   299 non-null    int64  \n",
      "dtypes: float64(3), int64(8)\n",
      "memory usage: 25.8 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "coated-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']  # These are all binary\n",
    "con_cols = ['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "individual-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_pipe = Pipeline([('scaler', StandardScaler()),  # Standard scaler for numerical variables\n",
    "                     ('imputer', SimpleImputer(strategy='median', add_indicator=True))])\n",
    "\n",
    "\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent'))])  # Shouldn't standardize binary variables\n",
    "\n",
    "\n",
    "\n",
    "# Apply numerical and categorical pipeline to pre-processing step\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_cols), \n",
    "                                   ('continuous', con_pipe, con_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-guest",
   "metadata": {},
   "source": [
    "Additionally, when looking at the target variable Death_event, I saw evidence of slight imbalanced data as the percentage of death events in the target variable was around 32%. Therefore, I will make sure to use metrics that weight the classes so as to deal with this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tamil-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([203,  96]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-terror",
   "metadata": {},
   "source": [
    "# Algorithms & Search\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-lodging",
   "metadata": {},
   "source": [
    "To analyze which algorithm has the best performance I utilize automated algorithm and hyperparameter search and used f1_weighted as my evaluation metric (since it works well for classification). I chose the \"weighted\" varient as it will help with inbalanced data. When picking algorithms, I focused on a variety of ones that would be good for binary classification. Additionally, I provide details on the important parameters for each model that will be varied in `RandomizedSearchCV()`. \n",
    "\n",
    "The three algorithms I chose to compare were:\n",
    "\n",
    "1. `LogisticRegression()` - I chose this model because it is a linear model for *classification*, which is the scope of my research question in this case.\n",
    "    - Important parameters:\n",
    "     - `penalty` - Specifies the normalization used in the penalty term\n",
    "     \n",
    "     - `C` - Inverse of regularization strength was varied (smaller values result in stronger regularization)\n",
    "     \n",
    "     - `solver` - Algorithm that is used in hte optimization problem \n",
    "     \n",
    "     - `class_weight` - The weights associated with specific classes (if we have evidence of imbalanced data, setting to `balanced` may be helpful\n",
    "     \n",
    "     \n",
    "2. `RandomForestClassifier()` - I chose this model because fitting a number of decision tree classifiers on various sub-samples of the data will improve accuracy in predictions and reduce overfitting \n",
    "    - Important parameters:\n",
    "        - `n_estimators` - number of trees in the forest, important because adding more trees trained on different subsets of the data reduces variance\n",
    "        - `criterion` - function used to determined the effectiveness of a split\n",
    "        - `bootstrap` - whether or not to use bootstrap samples\n",
    "            - **bootstrapping** is introducing amnesia by training trees on only a portion of the data, weakens the trees to improve *generality*\n",
    "        - `min_samples_leaf` - the minimum amount of samples required to be in a leaf node, decreasing this can decrease the validation error\n",
    "        - `class_weight` - weights assigned to classes (if None, classes are assumed to have a weight of 1\n",
    "            - **balanced** - uses values of y to adjust weights inversely to class frequencies\n",
    "            - **balanced_subsample** - same as above, but weights are computed based on the bootstrap sample for every tree\n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beginning-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "smoking-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "frozen-ecology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01      , 0.56444444, 1.11888889, 1.67333333, 2.22777778,\n",
       "       2.78222222, 3.33666667, 3.89111111, 4.44555556, 5.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.01, 5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-shore",
   "metadata": {},
   "source": [
    "## Comparison of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-helena",
   "metadata": {},
   "source": [
    "For the comments on importance of hyperparameters see markdown cell under Algorithms and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "diverse-desktop",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7693136900586677,\n",
       " {'clf__n_estimators': 16,\n",
       "  'clf__min_samples_leaf': 6,\n",
       "  'clf__criterion': 'gini',\n",
       "  'clf__class_weight': 'balanced',\n",
       "  'clf__bootstrap': False,\n",
       "  'clf': RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
       "                         min_samples_leaf=6, n_estimators=16, n_jobs=-1)})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps = [('preprocessing', preprocessing),\n",
    "                         ('clf', DummyEstimator())])\n",
    "\n",
    "# pipe = Pipeline(steps=[()])\n",
    "\n",
    "\n",
    "search_space = [\n",
    "        {'clf': [LogisticRegression(n_jobs=-1)],\n",
    "            'clf__C': np.linspace(0.01, 5, 10),\n",
    "            'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'clf__class_weight': ['balanced', None],\n",
    "            'clf__penalty': ['l1', 'l2', 'elasticnet', 'none']},\n",
    "                  \n",
    "                  \n",
    "        {'clf': [RandomForestClassifier(n_jobs=-1)],\n",
    "            'clf__criterion': ['gini', 'entropy'],\n",
    "            'clf__min_samples_leaf': np.linspace(1, 10, 6, dtype=int),\n",
    "            'clf__bootstrap': [True, False],\n",
    "            'clf__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "            'clf__n_estimators': np.linspace(0, 200, 100, dtype=int)},\n",
    "                  \n",
    "                  ]\n",
    "    \n",
    "\n",
    "gs = RandomizedSearchCV(pipe, \n",
    "                    search_space, \n",
    "                    scoring='f1_weighted',  # Used to combat slight class imbalance\n",
    "                    n_iter=30,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1\n",
    "                    )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "best_model = gs.best_params_['clf']\n",
    "\n",
    "gs.best_score_, gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-mechanism",
   "metadata": {},
   "source": [
    "As one can see the best model based on the cross validation and randomized search across algorithms and their hyperparameters is a RandomForestClassifier. To see the ideal parameters of the other models, one could comment out the other algorithms in the search_space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "distant-proposal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7372124250181938,\n",
       " {'clf__n_estimators': 94,\n",
       "  'clf__min_samples_leaf': 6,\n",
       "  'clf__criterion': 'entropy',\n",
       "  'clf__class_weight': 'balanced_subsample',\n",
       "  'clf__bootstrap': True,\n",
       "  'clf': RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                         min_samples_leaf=6, n_estimators=94, n_jobs=-1)})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1_weighted',\n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "best_model = gs.best_params_['clf']\n",
    "\n",
    "gs.best_score_, gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-arrangement",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-baseline",
   "metadata": {},
   "source": [
    "Now that I have my ideal model based on automated hyperparameter search and model selection, next I look at a variety of evaluation metrics on the testing data to assess our models performance on the testing set. The first metric I looked at was the weighted f1 score since that is what I used for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "wooden-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [('preprocessing', preprocessing), \n",
    "                         ('clf', best_model)])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "electrical-stanford",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7705685618729097"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-anthropology",
   "metadata": {},
   "source": [
    "Perhaps a better metric to gauge my model's performance is `f1_beta`, which allows the user to tweak the beta parameter, adding more weight to precision or recall (beta < 1 favors precision, beta > 1 favors recall). In the scope of this problem, it would make sense to put more weight on recall. Therefore, this model should focus on reducing the amount of false negatives (we would rather tell someone they are at risk of a death event when they're not than miss identifying a person who has dies). For this reason, I chose to look at fbeta_score with a beta value greater than 1 (as this puts more emphasis on false positives) and will tell us how good the model's recall is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "hired-passport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7096774193548386"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(y_test, y_pred, beta=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-platinum",
   "metadata": {},
   "source": [
    "As one can see, focusing in on the recall, demonstrates that our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "smoking-annex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47, 13],\n",
       "       [ 8, 22]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "broken-ladder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6285714285714286"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22/(22+13)  # Precision (TP / (TP + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-character",
   "metadata": {},
   "source": [
    "This precision value means that of those we predicted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-approval",
   "metadata": {},
   "source": [
    "Based on the confusion matrix this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "overhead-boston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22/(22+8)  # Recall (TP / (TP + FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-greenhouse",
   "metadata": {},
   "source": [
    "This recall value means that of individuals in the test set who did die, our model was able to predict 73 % of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-shoot",
   "metadata": {},
   "source": [
    "Recall: $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-penguin",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "----\n",
    "To answer my question of which features are the best predictors of a death event I look at feature importance and select the 3 most influential features based on my ideal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acquired-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = SelectFromModel(best_model, max_features=4)\n",
    "\n",
    "\n",
    "fs.fit_transform(X_train, y_train)\n",
    "\n",
    "features_to_keep = fs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ideal-roller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'ejection_fraction', 'serum_creatinine', 'serum_sodium']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x, y in zip(list(X.columns), list(features_to_keep)) if y == True]  # These are the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-witness",
   "metadata": {},
   "source": [
    "As one can see from above, the most important features for predicting a death event are age, ejection_fraction, serum_creatine, and serum_sodium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "alone-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_red = X_train[important_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "unusual-monaco",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>serum_creatinine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>60.0</td>\n",
       "      <td>3964</td>\n",
       "      <td>62</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>62.0</td>\n",
       "      <td>231</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>59.0</td>\n",
       "      <td>280</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>59.0</td>\n",
       "      <td>129</td>\n",
       "      <td>45</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>42.0</td>\n",
       "      <td>86</td>\n",
       "      <td>35</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  creatinine_phosphokinase  ejection_fraction  serum_creatinine\n",
       "52   60.0                      3964                 62               6.8\n",
       "11   62.0                       231                 25               0.9\n",
       "84   59.0                       280                 25               1.0\n",
       "159  59.0                       129                 45               1.1\n",
       "222  42.0                        86                 35               1.1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "accepted-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_search_space = {'clf': [RandomForestClassifier(n_jobs=-1)],\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__min_samples_leaf': np.linspace(1, 10, 6, dtype=int),\n",
    "        'clf__bootstrap': [True, False],\n",
    "        'clf__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "        'clf__n_estimators': np.linspace(0, 200, 100, dtype=int)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "micro-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_cols_red = ['creatinine_phosphokinase', 'ejection_fraction', 'serum_creatine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "essential-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                       ('imputer', IterativeImputer()),\n",
    "                       ('clf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "killing-numbers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7506240356754738,\n",
       " {'clf__n_estimators': 193,\n",
       "  'clf__min_samples_leaf': 8,\n",
       "  'clf__criterion': 'entropy',\n",
       "  'clf__class_weight': 'balanced_subsample',\n",
       "  'clf__bootstrap': True,\n",
       "  'clf': RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                         min_samples_leaf=8, n_estimators=193, n_jobs=-1)})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1_weighted',\n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "gs.fit(X_train_red, y_train)\n",
    "\n",
    "best_model = gs.best_params_['clf']\n",
    "\n",
    "gs.best_score_, gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-windsor",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble          import RandomForestRegressor\n",
    "\n",
    "fs = SelectFromModel(RandomForestRegressor(), \n",
    "                     max_features=3)\n",
    "\n",
    "fs.fit_transform(X_train, y_train) # Note only 3 features are outputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-rwanda",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-diagram",
   "metadata": {},
   "source": [
    "Now that I have my ideal model based on automated hyperparameter search and model selection, next I look at a variety of evaluation metrics on the training data to assess our models performance on the training set. The first metric I looked at was the weighted f1 score since that is what I used for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "continental-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [('preprocessing', preprocessing), \n",
    "                         ('clf', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "digital-plain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991584936439413"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "desperate-detection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991584936439413"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-ivory",
   "metadata": {},
   "source": [
    "I chose to look at a fbeta_score metric with a beta=2. I chose this metric since in a business setting use of this ML model deals with human lives. Therefore, this model should focus on reducing the amount of false negatives (we would rather tell someone they are at risk of a death event when they're not than miss identifying a person who has a death event. For this reason, I chose to look at fbeta_score with a beta value greater than 1 (as this puts more emphasis on false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "lightweight-position",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8793103448275862"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(y_train, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-fifth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "secondary-updating",
   "metadata": {},
   "source": [
    "Another evaluation metric I chose to look at "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
