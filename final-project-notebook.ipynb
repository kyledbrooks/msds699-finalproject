{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rental-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nonprofit-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm             import SVC\n",
    "\n",
    "from sklearn.decomposition   import PCA\n",
    "\n",
    "import imblearn\n",
    "from   imblearn.pipeline          import make_pipeline # scikit-learn Pipeline does not work with imblearn\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, fbeta_score, f1_score\n",
    "\n",
    "\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-cameroon",
   "metadata": {},
   "source": [
    "# Data Science Research Question\n",
    "-----\n",
    "## Can we develop an ML model to predict whether or not a patient will have a death event based on common heart failure predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alike-faculty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0  75.0        0                       582         0                 20   \n",
       "1  55.0        0                      7861         0                 38   \n",
       "2  65.0        0                       146         0                 20   \n",
       "3  50.0        1                       111         0                 20   \n",
       "4  65.0        1                       160         1                 20   \n",
       "\n",
       "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                    1  265000.00               1.9           130    1   \n",
       "1                    0  263358.03               1.1           136    1   \n",
       "2                    0  162000.00               1.3           129    1   \n",
       "3                    0  210000.00               1.9           137    1   \n",
       "4                    0  327000.00               2.7           116    0   \n",
       "\n",
       "   smoking  time  DEATH_EVENT  \n",
       "0        0     4            1  \n",
       "1        0     6            1  \n",
       "2        1     7            1  \n",
       "3        0     7            1  \n",
       "4        0     8            1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/heart_failure_clinical_records_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-crest",
   "metadata": {},
   "source": [
    "First after very basic EDA, I summarize the features in this dataset, and how they relate to heart failure:\n",
    "- **Age** - age of patient in years\n",
    "- **Anaemia** - whether or not a patient has a decrease of red blood cells (boolean)\n",
    "- **Creatinine Phosphokinase** - level of CPK enzyme in blood in (high values may indicate muscle damage - heart is a muscle)\n",
    "- **Diabetes** - whether or not a patient has diabetes (boolean)\n",
    "- **Ejection Fraction** - percentage of blood being pumped out of left ventricle (lower values may indicate issues)\n",
    "- **High Blood Pressure** - whether or not a patient has hypertension (boolean)\n",
    "- **Platelets** - concentration of platelets in blood in kiloplatelets/mL\n",
    "- **Serum Creatinine** - concentration of creatinine in blood in mg/dL (increased levels are a marker of poor cardiac output).\n",
    "- **Serum Sodium** - level of sodium in blood in mEq/L (low concentration is a biological marker for heart failure)\n",
    "- **Sex** - whether the patient is male or female (binary: 1=Male, 0=Female)\n",
    "- **Smoking** - if the patient smokes (boolean)\n",
    "- **Time** - the follow up period for the patient up to the death event (whether they died or were censored).\n",
    "- **Death Event** - (target), whether or not the patient died during the follow up (boolean)\n",
    "    - 1 - patient died\n",
    "    - 0 - patient was censored (scientist lost contact with patient) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-fault",
   "metadata": {},
   "source": [
    "Since the time variable is populated when the scientists either lost contact with the patient, or the patient died, it is indicative of survival, and obviously not known beforehand. In other words, if we we're to deploy a working model with all the columns and someone wanted to predict whether a certain patient was likely to have a death event, they would have no \"time\" variable to put as an input (since that is recorded when the patient dies). Therefore, I made the decision initially to drop time as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mounted-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.DEATH_EVENT # Our target variable is DEATH_EVENT \n",
    "X = data.drop(columns=['DEATH_EVENT', 'time'])  # Remove the target variable and time from X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "congressional-engine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many observations do we have?\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-darkness",
   "metadata": {},
   "source": [
    "After importing the data, the next step is to pull off a segment of the data that will be our testing set. This test set will be be hidden away from the models we are designing until we are ready to test one. Additionally, since there are only have 299 observations, I am going to split the data by a slightly higher percentage (25% as opposed to 20%) to ensure that the testing set is somewhat representative of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "critical-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing split that we are going to hide away from our model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-juvenile",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-glucose",
   "metadata": {},
   "source": [
    "After performing more extensive EDA (in a different notebook), I noticed that all of the columns were numeric types, but some represented categorical variables (i.e. whether or not someone smokes, has high blood pressure, etc.). In the dataset, these columns are already one-hot-encoded with binary 1s or 0s to indicate whether someone exhibits that characteristic. Therefore, in my feature engineering one-hot-encoding is not needed. Additionally, there are no nan values in the training data, however I still impute for missing values, as they might be included in the testing set or data we want to use the model on.\n",
    "\n",
    "For my feature engineering, the steps I took were:\n",
    "1. Identify categorical and numerical columns and establish pipelines for each\n",
    "\n",
    "\n",
    "2. Scale the data\n",
    "    - For my numeric data I chose to use StandardScaler(), which normalizes columns to fall in line with the normal distribution. This is important as many algorithms may perform poorly if individual features do not look more or less like a standard normally distributed data.\n",
    "    - It doesn't make sense to use a scaler on my categorical columns since they're simply one-hot-encoded as 1s or 0s.\n",
    "\n",
    "\n",
    "3. Impute missing values with SimpleImputer()\n",
    "    - For numerical columns I used the strategy \"median\" whereas for categorical I used \"most_frequent\". These methods fill any missing values with the corresponding strategy applied to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "catholic-friendly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299 entries, 0 to 298\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       299 non-null    float64\n",
      " 1   anaemia                   299 non-null    int64  \n",
      " 2   creatinine_phosphokinase  299 non-null    int64  \n",
      " 3   diabetes                  299 non-null    int64  \n",
      " 4   ejection_fraction         299 non-null    int64  \n",
      " 5   high_blood_pressure       299 non-null    int64  \n",
      " 6   platelets                 299 non-null    float64\n",
      " 7   serum_creatinine          299 non-null    float64\n",
      " 8   serum_sodium              299 non-null    int64  \n",
      " 9   sex                       299 non-null    int64  \n",
      " 10  smoking                   299 non-null    int64  \n",
      "dtypes: float64(3), int64(8)\n",
      "memory usage: 25.8 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "coated-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']  # These are all binary\n",
    "con_cols = ['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "individual-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_pipe = Pipeline([('scaler', StandardScaler()),  # Standard scaler for numerical variables\n",
    "                     ('imputer', SimpleImputer(strategy='median', add_indicator=True))])\n",
    "\n",
    "\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent'))])  # Shouldn't standardize binary variables\n",
    "\n",
    "\n",
    "\n",
    "# Apply numerical and categorical pipeline to pre-processing step\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_cols), \n",
    "                                   ('continuous', con_pipe, con_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "clinical-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con_pipe = make_pipeline(imblearn.over_sampling.SMOTE(), StandardScaler(), SimpleImputer(strategy='median'))\n",
    "\n",
    "# age_pipe = make_pipeline(imblearn.over_sampling.SMOTE(), MinMaxScaler((0,1)), SimpleImputer(strategy='median'))\n",
    "\n",
    "# cat_pipe = make_pipeline(imblearn.over_sampling.SMOTE(), SimpleImputer(strategy='most_frequent'))\n",
    "\n",
    "# preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_cols),\n",
    "#                                    ('continuous', con_pipe, con_cols),\n",
    "#                                    ('age', age_pipe, age_col)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-guest",
   "metadata": {},
   "source": [
    "Additionally, when looking at the target variable Death_event, I saw evidence of imbalanced data as the percentage of death events in the target variable was around 32%. Therefore, in my pipeline for the randomized search CV, I will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tamil-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([203,  96]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-terror",
   "metadata": {},
   "source": [
    "# Algorithms & Search\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-lodging",
   "metadata": {},
   "source": [
    "To analyze which algorithm has the best performance I utilize automated algorithm and hyperparameter search and used f1 as my evaluation metric (since it works well for classification). When picking algorithms, I focused on a variety of ones that would be good for binary classification. The three algorithms I chose to compare were\n",
    "\n",
    "1. `LogisticRegression()` - I chose this model because it is a linear model for *classification*, which is the scope of my research question in this case.\n",
    "    - Important parameters:\n",
    "     - `penalty` - Specifies the normalization used in the penalty term\n",
    "     \n",
    "     - `C` - Inverse of regularization strength was varied (smaller values result in stronger regularization)\n",
    "     \n",
    "     - `solver` - Algorithm that is used in hte optimization problem \n",
    "     \n",
    "     - `class_weight` - The weights associated with specific classes (if we have evidence of imbalanced data, setting to `balanced` may be helpful\n",
    "     \n",
    "     \n",
    "2. `RandomForestClassifier()` - I chose this model because fitting a number of decision tree classifiers on various sub-samples of the data will improve accuracy in predictions and reduce overfitting \n",
    "    - Important parameters:\n",
    "        - `n_estimators` - number of trees in the forest, important because adding more trees trained on different subsets of the data reduces variance\n",
    "        - `criterion` - function used to determined the effectiveness of a split\n",
    "        - `bootstrap` - whether or not to use bootstrap samples\n",
    "            - **bootstrapping** is introducing amnesia by training trees on only a portion of the data, weakens the trees to improve *generality*\n",
    "        - `min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-trigger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beginning-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "smoking-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "frozen-ecology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01      , 0.56444444, 1.11888889, 1.67333333, 2.22777778,\n",
       "       2.78222222, 3.33666667, 3.89111111, 4.44555556, 5.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.01, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "diverse-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe = make_pipeline(imblearn.over_sampling.SMOTE(),\n",
    "#                      preprocessing,\n",
    "#                      PCA(),\n",
    "#                      DummyEstimator()\n",
    "#                      )\n",
    "\n",
    "pipe = Pipeline(steps = [('smote', imblearn.over_sampling.SMOTE()),\n",
    "                         ('preprocessing', preprocessing),\n",
    "                         ('clf', DummyEstimator())])\n",
    "\n",
    "# pipe = Pipeline(steps=[()])\n",
    "\n",
    "\n",
    "search_space = [\n",
    "    {'clf': [LogisticRegression(n_jobs=-1)],\n",
    "        'clf__C': np.linspace(0.01, 5, 10),\n",
    "        'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'clf__class_weight': ['balanced', None],\n",
    "        'clf__penalty': ['l1', 'l2', 'elasticnet', 'none']},\n",
    "    \n",
    "    {'clf': [RandomForestClassifier(n_jobs=-1)],\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__min_samples_leaf': np.linspace(1, 10, 6, dtype=int),\n",
    "        'clf__bootstrap': [True, False],\n",
    "        'clf__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "        'clf__n_estimators': np.linspace(0, 200, 100, dtype=int)},\n",
    "    \n",
    "    {'clf': [SVC()], \n",
    "         'clf__class_weight': ['balanced', None],\n",
    "         'clf__C': np.linspace(1, 100, 10),\n",
    "         'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "         'clf__degree': range(1,10)}, \n",
    "    \n",
    "    {'clf': [KNeighborsClassifier(n_jobs=-1)],\n",
    "         'clf__leaf_size': np.linspace(5, 50, 5, dtype=int),\n",
    "         'clf__n_neighbors': np.linspace(3, 13, 4, dtype=int),\n",
    "         'clf__weights': ['uniform', 'distance'],\n",
    "         'clf__p': [1,2]}, \n",
    "    \n",
    "    {'clf': [GaussianNB()]}, \n",
    "    \n",
    "    {'clf': [ExtraTreesClassifier(n_jobs=-1)], \n",
    "         'clf__criterion': ['gini', 'entropy'],\n",
    "         'clf__min_samples_leaf': np.linspace(1, 30, 5, dtype=int),\n",
    "         'clf__bootstrap': [True, False],\n",
    "         'clf__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "         'clf__n_estimators': np.linspace(0, 500, 100, dtype=int)}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "distant-proposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7760376301744614,\n",
       " {'clf__n_estimators': 6,\n",
       "  'clf__min_samples_leaf': 10,\n",
       "  'clf__criterion': 'entropy',\n",
       "  'clf__class_weight': 'balanced_subsample',\n",
       "  'clf__bootstrap': False,\n",
       "  'clf': RandomForestClassifier(bootstrap=False, class_weight='balanced_subsample',\n",
       "                         criterion='entropy', min_samples_leaf=10, n_estimators=6,\n",
       "                         n_jobs=-1)})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1_weighted', \n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "best_model = gs.best_params_['clf']\n",
    "\n",
    "gs.best_score_, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "excited-kitchen",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n",
      "    self._validate_estimator()\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/ensemble/_base.py\", line 134, in _validate_estimator\n",
      "    raise ValueError(\"n_estimators must be greater than zero, \"\n",
      "ValueError: n_estimators must be greater than zero, got 0.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n",
      "    self._validate_estimator()\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/ensemble/_base.py\", line 134, in _validate_estimator\n",
      "    raise ValueError(\"n_estimators must be greater than zero, \"\n",
      "ValueError: n_estimators must be greater than zero, got 0.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 348, in fit\n",
      "    self._validate_estimator()\n",
      "  File \"/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/ensemble/_base.py\", line 134, in _validate_estimator\n",
      "    raise ValueError(\"n_estimators must be greater than zero, \"\n",
      "ValueError: n_estimators must be greater than zero, got 0.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={'n_estimators': array([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,\n",
       "        26,  28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,\n",
       "        52,  54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,\n",
       "        78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100, 102,\n",
       "       104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128,\n",
       "       130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154,\n",
       "       156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180,\n",
       "       182, 184, 186, 188, 190, 192, 194, 196, 198])})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = {\"n_estimators\": np.arange(0,200,2)}\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = GridSearchCV(rf, grid, cv=3)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "vertical-fault",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 122}\n",
      "RandomForestClassifier(n_estimators=122)\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_params_)\n",
    "print(rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "regulated-championship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "breathing-robert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5180524614688842"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_list = []\n",
    "for i in range(10):\n",
    "    gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1', \n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    score_list.append(gs.best_score_)\n",
    "\n",
    "with_smote = np.mean(score_list)\n",
    "\n",
    "with_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "academic-physiology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.519672000277809"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_list = []\n",
    "for i in range(10):\n",
    "    gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1', \n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    score_list.append(gs.best_score_)\n",
    "\n",
    "without_smote = np.mean(score_list)\n",
    "\n",
    "without_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "departmental-calendar",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5244444444444445,\n",
       " {'clf__n_estimators': 50,\n",
       "  'clf__min_samples_leaf': 4,\n",
       "  'clf__criterion': 'gini',\n",
       "  'clf__class_weight': None,\n",
       "  'clf__bootstrap': True,\n",
       "  'clf': RandomForestClassifier(min_samples_leaf=4, n_estimators=50, n_jobs=-1)})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1', \n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "gs.best_score_, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "warming-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_list = [imblearn.over_sampling.SMOTE(), imblearn.over_sampling.ADASYN(), imblearn.over_sampling.BorderlineSMOTE(), imblearn.over_sampling.SVMSMOTE(),\n",
    "            imblearn.combine.SMOTEENN(), imblearn.combine.SMOTETomek()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "faced-variety",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE() - 0.5308903680015374\n",
      "ADASYN() - 0.5362001898682767\n",
      "BorderlineSMOTE() - 0.5145619642502345\n",
      "SVMSMOTE() - 0.5105900477783913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylebrooks/opt/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1351: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN() - 0.48616156601531335\n",
      "SMOTETomek() - 0.5305956924570111\n"
     ]
    }
   ],
   "source": [
    "for imb in imb_list:\n",
    "    total = 0\n",
    "    for i in range(10):\n",
    "        pipe = Pipeline(steps = [('imb', imb),\n",
    "                         ('preprocessing', preprocessing),\n",
    "                         ('clf', DummyEstimator())])\n",
    "\n",
    "        gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1', \n",
    "                        n_iter=30,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "        gs.fit(X_train, y_train)\n",
    "\n",
    "        total += gs.best_score_\n",
    "        \n",
    "    avg = total / 10\n",
    "    \n",
    "    print(f'{imb} - {avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "mediterranean-saturday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced_subsample', min_samples_leaf=10,\n",
       "                       n_estimators=300, n_jobs=-1)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gs.best_params_['dummyestimator']\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-rwanda",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-diagram",
   "metadata": {},
   "source": [
    "Now that I have our ideal model based on automated hyperparameter search and model selection, next I look at a variety of evaluation metrics on the training data to assess our models performance on the training set. The first metric I looked at was the weighted f1 score since that is what I used for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "continental-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [('preprocessing', preprocessing), \n",
    "                         ('clf', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "digital-plain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991584936439413"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "desperate-detection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991584936439413"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-ivory",
   "metadata": {},
   "source": [
    "I chose to look at a fbeta_score metric with a beta=2. I chose this metric since in a business setting use of this ML model deals with human lives. Therefore, this model should focus on reducing the amount of false negatives (we would rather tell someone they are at risk of a death event when they're not than miss identifying a person who has a death event. For this reason, I chose to look at fbeta_score with a beta value greater than 1 (as this puts more emphasis on false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "lightweight-position",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8793103448275862"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(y_train, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-fifth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "secondary-updating",
   "metadata": {},
   "source": [
    "Another evaluation metric I chose to look at "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
